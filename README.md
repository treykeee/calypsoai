<div align="center">

# üõ°Ô∏è LLM Security and AI Security Repository

</div>

<div align="center">

## üöÄ Get in Touch with Calypso AI

- [**üê¶ Follow us on Twitter**](https://twitter.com/calypsoai?lang=en)
- [**üåê Explore our Website**](https://calypsoai.com/)
- [**üîó Connect with us on LinkedIn**](https://www.linkedin.com/company/calypso-ai/)

</div>

## üåü Introduction

*LLM (Large Language Models)* and *AI (Artificial Intelligence)* are rapidly evolving fields that have the potential to revolutionize the way we live and work. However, with this potential comes the need for increased security measures to protect against potential threats and vulnerabilities. In this document, we will explore the various security features of LLM and AI, and how they can be used to enhance security and protect against potential threats.

## **üîí Section 1: LLM Security**


- **üå™Ô∏è Adversarial training**
1. Goodfellow, I., Shlens, J., & Szegedy, C. (2015). [Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.](https://arxiv.org/abs/1412.6572)
2. Kurakin, A., Goodfellow, I., & Bengio, S. (2016). [Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533.](https://arxiv.org/abs/1607.02533)
3. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., & Vladu, A. (2018). [Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.](https://arxiv.org/abs/1706.06083)


- **üßπ Data sanitization**
1. Blancco Technology Group. (2019). [The Critical Importance of Data Sanitization.](https://www.blancco.com/resources/)
2. National Institute of Standards and Technology. (2019). [Guidelines for Media Sanitization.](https://csrc.nist.gov/publications/detail/sp/800-88/rev-1/final)


- **üì¶ Model compression**
1. H. Xue and K. Ren, "Recent research trends on Model Compression and Knowledge Transfer in CNNs," 2021 IEEE International Conference on Computer Science, Artificial Intelligence and Electronic Engineering (CSAIEE), SC, USA, 2021, pp. 136-142, doi: 10.1109/CSAIEE54046.2021.9543192, [https://ieeexplore.ieee.org/document/9543192](https://ieeexplore.ieee.org/document/9543192).
2. Li, Z.; Li, H.; Meng, L. Model Compression for Deep Neural Networks: A Survey. Computers 2023, 12, 60. [https://doi.org/10.3390/computers12030060](https://doi.org/10.3390/computers12030060)
3. Giosu√© Cataldo Marin√≥, Alessandro Petrini, Dario Malchiodi, Marco Frasca. Deep neural networks compression: A comparative survey and choice recommendations. [https://www.sciencedirect.com/science/article/pii/S0925231222014643](https://www.sciencedirect.com/science/article/pii/S0925231222014643)
4. Mary Shanthi Rani M, Chitra P, Lakshmanan S, Kalpana Devi M, Sangeetha R, Nithya S. DeepCompNet: A Novel Neural Net Model Compression Architecture. Comput Intell Neurosci. 2022 Feb 22;2022:2213273. doi: 10.1155/2022/2213273. PMID: 35242176; PMCID: PMC8888078. [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8888078/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8888078/)


- **üîè Differential privacy**
1. Jain, P., Gyanchandani, M. & Khare, N. Differential privacy: it's technological prescriptive using big data. J Big Data 5, 15 (2018). [https://doi.org/10.1186/s40537-018-0124-9](https://doi.org/10.1186/s40537-018-0124-9)
2. Joseph Ficek, Wei Wang, Henian Chen, Getachew Dagne, Ellen Daley, Differential privacy in health research: A scoping review, Journal of the American Medical Informatics Association, Volume 28, Issue 10, October 2021, Pages 2269‚Äì2276, [https://doi.org/10.1093/jamia/ocab135](https://doi.org/10.1093/jamia/ocab135)
3. Alberto Blanco-Justicia, David S√°nchez, Josep Domingo-Ferrer, and Krishnamurty Muralidhar. 2022. A Critical Review on the Use (and Misuse) of Differential Privacy in Machine Learning. ACM Comput. Surv. 55, 8, Article 160 (August 2023), 16 pages. [https://doi.org/10.1145/3547139](https://doi.org/10.1145/3547139)
4. Dyda A, Purcell M, Curtis S, Field E, Pillai P, Ricardo K, Weng H, Moore JC, Hewett M, Williams G, Lau CL. Differential privacy for public health data: An innovative tool to optimize information sharing while protecting data confidentiality. Patterns (N Y). 2021 Dec 10;2(12):100366. doi: 10.1016/j.patter.2021.100366. PMID: 34909703; PMCID: PMC8662814. [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8662814/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8662814/)




## **üõ°Ô∏è Section 2: AI Security**


- **üìñ Explainability**
1. Saranya A., Subhashini R. A systematic review of Explainable Artificial Intelligence models and applications: Recent developments and future trends. School of Information Technology and Engineering, VIT University, Vellore, India. [https://doi.org/10.1016/j.dajour.2023.100230](https://doi.org/10.1016/j.dajour.2023.100230)
2. Chaddad A, Peng J, Xu J, Bouridane A. Survey of Explainable AI Techniques in Healthcare. Sensors (Basel). 2023 Jan 5;23(2):634. doi: 10.3390/s23020634. PMID: 36679430; PMCID: PMC9862413. [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9862413/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9862413/)
3. Linardatos P, Papastefanopoulos V, Kotsiantis S. Explainable AI: A Review of Machine Learning Interpretability Methods. Entropy (Basel). 2020 Dec 25;23(1):18. doi: 10.3390/e23010018. PMID: 33375658; PMCID: PMC7824368. [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7824368/)
4. Charmet, F., Tanuwidjaja, H.C., Ayoubi, S. et al. Explainable artificial intelligence for cybersecurity: a literature survey. Ann. Telecommun. 77, 789‚Äì812 (2022). [https://doi.org/10.1007/s12243-022-00926-7](https://doi.org/10.1007/s12243-022-00926-7)
5. Guembe, Blessing and Azeta, Ambrose and Osamor, Victor and Ekpo, Raphael, Explainable Artificial Intelligence, the Fourth Pillar of Zero Trust Security (November 23, 2022). Available at SSRN: [https://ssrn.com/abstract=4331547](https://ssrn.com/abstract=4331547) or [http://dx.doi.org/10.2139/ssrn.4331547](http://dx.doi.org/10.2139/ssrn.4331547)


- **üèãÔ∏è Robustness**
1. Hito M, Wang W, Stephens H, Xie Y, Li R, Yin FF, Ge Y, Wu QJ, Wu Q, Sheng Y. Assessing the robustness of artificial intelligence powered planning tools in radiotherapy clinical settings-a phantom simulation approach. Quant Imaging Med Surg. 2021 Dec;11(12):4835-4846. doi: 10.21037/qims-21-51. PMID: 34888193; PMCID: PMC8611457. [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8611457/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8611457/)
2. Rui Chen, Meiling Wang, Yi Lai. Analysis of the role and robustness of artificial intelligence in commodity image recognition under deep learning neural network. Published: July 7, 2020. [https://doi.org/10.1371/journal.pone.0235783](https://doi.org/10.1371/journal.pone.0235783)
3. Ghaffari Laleh, N., Truhn, D., Veldhuizen, G.P. et al. Adversarial attacks and adversarial robustness in computational pathology. Nat Commun 13, 5711 (2022). [https://doi.org/10.1038/s41467-022-33266-0](https://doi.org/10.1038/s41467-022-33266-0)
4. P. Chen and P. Das, "AI Maintenance: A Robustness Perspective" in Computer, vol. 56, no. 02, pp. 48-56, 2023. doi: 10.1109/MC.2022.3218005. [https://doi.ieeecomputersociety.org/10.1109/MC.2022.3218005](https://doi.ieeecomputersociety.org/10.1109/MC.2022.3218005)


- **üïµÔ∏è Privacy**
1. Murdoch, B. Privacy and artificial intelligence: challenges for protecting health information in a new era. BMC Med Ethics 22, 122 (2021). [https://doi.org/10.1186/s12910-021-00687-3](https://doi.org/10.1186/s12910-021-00687-3)
2. Elliott D, Soifer E. AI Technologies, Privacy, and Security. Front Artif Intell. 2022 Apr 13;5:826737. doi: 10.3389/frai.2022.826737. PMID: 35493613; PMCID: PMC9044077. [https://doi.org/10.3389/frai.2022.826737](https://doi.org/10.3389/frai.2022.826737)
3. Sare Baase and Timothy M. Henry. A Gift of Fire Social, Legal, and Ethical Issues for Computing Technology (5th Edition). Prentice Hall PTR, 2017. ISBN 9780134615271. URL [http://www.worldcat.org/oclc/1050275090](http://www.worldcat.org/oclc/1050275090) (Chap. 2).
4. Bartneck, C., L√ºtge, C., Wagner, A., Welsh, S. (2021). Privacy Issues of AI. In: An Introduction to Ethics in Robotics and AI. SpringerBriefs in Ethics. Springer, Cham. [https://doi.org/10.1007/978-3-030-51110-4_8](https://doi.org/10.1007/978-3-030-51110-4_8)
5. Honghao Gao, Zhiyuan Tan, Special Issue on Adversarial AI to IoT Security and Privacy Protection: Attacks and Defenses, The Computer Journal, Volume 65, Issue 11, November 2022, Pages 2847‚Äì2848, [https://doi.org/10.1093/comjnl/bxac128](https://doi.org/10.1093/comjnl/bxac128)


- **üìù Accountability**
1. Novelli, C., Taddeo, M. & Floridi, L. Accountability in artificial intelligence: what it is and how it works. AI & Soc (2023). [https://doi.org/10.1007/s00146-023-01635-y](https://doi.org/10.1007/s00146-023-01635-y)
2. Hohma E, Boch A, Trauth R, L√ºtge C. Investigating accountability for Artificial Intelligence through risk governance: A workshop-based exploratory study. Front Psychol. 2023 Jan 25;14:1073686. doi: 10.3389/fpsyg.2023.1073686. PMID: 36760454; PMCID: PMC9905430. [https://doi.org/10.3389/fpsyg.2023.1073686](https://doi.org/10.3389/fpsyg.2023.1073686)
3. Ashwin Kumar Raja, Jianlong Zhou. AI Accountability: Approaches, Affecting Factors, and Challenges. IEEE Computer, vol. 56, pp. 61-70, April 2023. [10.1109/MC.2023.3238390](https://doi.org/10.1109/MC.2023.3238390)
4. Busuioc, M. (2021), Accountable Artificial Intelligence: Holding Algorithms to Account. Public Admin Rev, 81: 825-836. [https://doi.org/10.1111/puar.13293](https://doi.org/10.1111/puar.13293)
5. Raji, I. D., Smart, A., White, R. N., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., & Barnes, P. Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing. FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, January 2020, Pages 33‚Äì44. [https://doi.org/10.1145/3351095.3372873](https://doi.org/10.1145/3351095.3372873)
6. Heine, K., & Quintavalla, A. (2023). Bridging the accountability gap of artificial intelligence ‚Äì what can be learned from Roman law? Legal Studies, 1-16. doi: [10.1017/lst.2022.51](https://doi.org/10.1017/lst.2022.51)




##
AI and LLMs are powerful tools that have the potential to revolutionize the way we live and work. However, with this potential comes the need for increased security measures to protect against potential threats and vulnerabilities.



## **üåê Contact**

- [**üê¶ Twitter**](https://twitter.com/calypsoai?lang=en)
- [**üåç Website**](https://calypsoai.com/)
- [**üîó LinkedIn**](https://www.linkedin.com/company/calypso-ai/)
